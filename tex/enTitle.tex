% !TeX root=../main.tex
% در این فایل، عنوان پایان‌نامه، مشخصات خود و چکیده پایان‌نامه را به انگلیسی، وارد کنید.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\latinuniversity{University of Tehran}
\latincollege{College of Engineering}
\latinfaculty{Faculty of Engineering Science}
\latindepartment{Software Engineering}
\latinsubject{Computer Engineering}
\latinfield{Software Engineering}
\latintitle{Automatically Generate Testcases for Financial Transaction Processing Systems Based on Adaptive Random Testing}
\firstlatinsupervisor{Dr. Ramtin Khosravi}
%\secondlatinsupervisor{Second Supervisor}
%\firstlatinadvisor{First Advisor}
%\secondlatinadvisor{Second Advisor}
\latinname{Mehdi}
\latinsurname{Khorsand}
\latinthesisdate{September 2024}
\latinkeywords{
	Software Testing, 
	Auto Generate Test Cases, 
	Random Testing, 
	Adaptive Random Testing
	}
\en-abstract{
Adaptive random testing method was introduced in 2001 by Chen et al. to address the shortcomings of random testing, such as repeatability issues, inefficiency, and limited focus. Initially, this method was focused on software with numerical input domains and aimed to resolve the problems of random testing by selecting dispersed test cases across the input domain.
In this method, for each new test case selection, several candidate test cases are generated randomly, and the one with the greatest difference from the previously selected test cases is chosen. The difference between test cases is first calculated for numerical domains by mapping the software's inputs into a multidimensional space, followed by measuring the distance between the corresponding points of the test cases.
Due to the better results this method demonstrated compared to random testing in numerical domains, many studies were conducted on the implementation of adaptive random testing for non-numerical domains with object-oriented inputs. The common point of all these methods is offering a solution to map object-oriented test cases into one or multiple multidimensional spaces using the inputs and outputs of the system under test during the execution of the tests, as well as proposing a strategy to reduce the cost of test case selection when the number of test cases is high. Since the difference between each candidate test case and every previously selected test case must be calculated, the selection of a new test case becomes costly as the number of previously selected test cases grows.
In this research, a method is proposed to calculate the difference between test cases based on a new scoring strategy for behaviors observed by the system under test during test execution. This method not only calculates the difference between two test cases but also is capable to calculate the difference between a test case and a set of other test cases, or even between two separate sets of test cases. The cost of selecting a new test case in this method is independent of the number of previously selected test cases and it demonstrates better performance in terms of coverage and fault detection compared to other test case selection methods in adaptive random testing.
}
